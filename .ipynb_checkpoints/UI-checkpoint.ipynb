{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52df76e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76319672-01bc-40e9-9283-8d2d7914aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc22688c-4bc2-468f-9273-ea175402554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_preprocess_images(frame):\n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Load Haar cascade classifiers for face and eye detection\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # Initialize lists to store cropped faces, original images, left eyes, and right eyes\n",
    "    cropped_faces = []\n",
    "    original_images = []\n",
    "    left_eyes = []\n",
    "    right_eyes = []\n",
    "\n",
    "    # Iterate over detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Crop the face region from the frame\n",
    "        cropped_face = frame[y:y+h, x:x+w]\n",
    "        cropped_faces.append(cropped_face)\n",
    "\n",
    "        # Convert cropped face to grayscale\n",
    "        #cropped_face_gray = cv2.cvtColor(cropped_face, cv2.COLOR_BGR2GRAY)\n",
    "        #cropped_faces_gray.append(cropped_face_gray)\n",
    "\n",
    "        # Append original frame to the original_images list\n",
    "        original_images.append(gray[y:y+h, x:x+w])\n",
    "\n",
    "        # Detect eyes within the face region\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        \n",
    "        # Iterate over detected eyes\n",
    "        for i, (ex, ey, ew, eh) in enumerate(eyes):\n",
    "            # Crop eye region from the face\n",
    "            eye_roi = cropped_face[ey:ey+eh, ex:ex+ew]\n",
    "            if i == 0:\n",
    "                left_eyes.append(eye_roi)\n",
    "            elif i == 1:\n",
    "                right_eyes.append(eye_roi)\n",
    "\n",
    "    # Resize and normalize images\n",
    "    cropped_faces_gray = [cv2.resize(img, (227, 227)) for img in cropped_faces_gray]\n",
    "    original_images = [cv2.resize(img, (227, 227)) for img in original_images]\n",
    "    left_eyes = [cv2.resize(img, (30, 30)) for img in left_eyes]\n",
    "    right_eyes = [cv2.resize(img, (30, 30)) for img in right_eyes]\n",
    "\n",
    "    cropped_faces_gray = [img / 255.0 for img in cropped_faces_gray]\n",
    "    original_images = [img / 255.0 for img in original_images]\n",
    "    left_eyes = [img / 255.0 for img in left_eyes]\n",
    "    right_eyes = [img / 255.0 for img in right_eyes]\n",
    "\n",
    "    return cropped_faces, original_images, left_eyes, right_eyes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec828054-58d6-4092-960d-923a8b9e5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_preprocess_images_grey(frame):\n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Load Haar cascade classifiers for face and eye detection\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    # Initialize lists to store cropped faces, original images, left eyes, and right eyes\n",
    "    cropped_faces_gray = []\n",
    "    original_images = []\n",
    "    left_eyes_gray = []\n",
    "    right_eyes_gray = []\n",
    "\n",
    "    # Iterate over detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Crop the face region from the frame\n",
    "        cropped_face = frame[y:y+h, x:x+w]\n",
    "        cropped_faces_gray.append(cv2.cvtColor(cropped_face, cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "        # Append original frame to the original_images list\n",
    "        original_images.append(gray[y:y+h, x:x+w])\n",
    "\n",
    "        # Detect eyes within the face region\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        \n",
    "        # Iterate over detected eyes\n",
    "        for i, (ex, ey, ew, eh) in enumerate(eyes):\n",
    "            # Crop eye region from the face\n",
    "            eye_roi = roi_color[ey:ey+eh, ex:ex+ew]\n",
    "            eye_gray = cv2.cvtColor(eye_roi, cv2.COLOR_BGR2GRAY)\n",
    "            if i == 0:\n",
    "                left_eyes_gray.append(eye_gray)\n",
    "            elif i == 1:\n",
    "                right_eyes_gray.append(eye_gray)\n",
    "\n",
    "    # Resize and normalize images\n",
    "    cropped_faces_gray = [cv2.resize(img, (227, 227)) for img in cropped_faces_gray]\n",
    "    original_images = [cv2.resize(img, (227, 227)) for img in original_images]\n",
    "    left_eyes_gray = [cv2.resize(img, (30, 30)) for img in left_eyes_gray]\n",
    "    right_eyes_gray = [cv2.resize(img, (30, 30)) for img in right_eyes_gray]\n",
    "\n",
    "    cropped_faces_gray = [img / 255.0 for img in cropped_faces_gray]\n",
    "    original_images = [img / 255.0 for img in original_images]\n",
    "    left_eyes_gray = [img / 255.0 for img in left_eyes_gray]\n",
    "    right_eyes_gray = [img / 255.0 for img in right_eyes_gray]\n",
    "\n",
    "    return cropped_faces_gray, original_images, left_eyes_gray, right_eyes_gray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7d88984",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'cropped_faces_gray' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m \n\u001b[1;32m---> 17\u001b[0m cropped_faces, original_images, left_eyes, right_eyes \u001b[38;5;241m=\u001b[39m extract_and_preprocess_images(frame)\n\u001b[0;32m     19\u001b[0m points \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([original_images, cropped_faces, left_eyes, right_eyes])\n\u001b[0;32m     20\u001b[0m points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(points)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 26\u001b[0m, in \u001b[0;36mextract_and_preprocess_images\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Convert cropped face to grayscale\u001b[39;00m\n\u001b[0;32m     25\u001b[0m cropped_face_gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(cropped_face, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m---> 26\u001b[0m cropped_faces_gray\u001b[38;5;241m.\u001b[39mappend(cropped_face_gray)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Append original frame to the original_images list\u001b[39;00m\n\u001b[0;32m     29\u001b[0m original_images\u001b[38;5;241m.\u001b[39mappend(gray[y:y\u001b[38;5;241m+\u001b[39mh, x:x\u001b[38;5;241m+\u001b[39mw])\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'cropped_faces_gray' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "x1, y1 = 150, 150  \n",
    "x2, y2 = 550, 550\n",
    "grid_points = cv2.imread('grid_points.jpg', 1)\n",
    "radius = 10 \n",
    "color = (0, 255, 0)  # Green color (BGR format)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    frame = frame[y1:y2, x1:x2]\n",
    "    \n",
    "    if not ret:\n",
    "        break \n",
    "\n",
    "    \n",
    "    cropped_faces, original_images, left_eyes, right_eyes = extract_and_preprocess_images(frame)\n",
    "    \n",
    "    points = model.predict([original_images, cropped_faces, left_eyes, right_eyes])\n",
    "    points = np.round(points).astype(int)\n",
    "    points = points[0]\n",
    "    for point in points:\n",
    "        x, y = point\n",
    "\n",
    "        cv2.circle(grid_points, (int(x), int(y)), radius, color, -1)\n",
    "    \n",
    "    cv2.imshow('Test Image with Circle', grid_points)    \n",
    "    cv2.imshow('Video Feed', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eb4a6e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 227, 227, 30, 30\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Check if all four images are present\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cropped_faces \u001b[38;5;129;01mand\u001b[39;00m original_images \u001b[38;5;129;01mand\u001b[39;00m left_eyes \u001b[38;5;129;01mand\u001b[39;00m right_eyes:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Predict gaze points using the model\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     points \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([original_images, cropped_faces, left_eyes, right_eyes])\n\u001b[0;32m     35\u001b[0m     points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(points)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     36\u001b[0m     points \u001b[38;5;241m=\u001b[39m points[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\data_adapter_utils.py:114\u001b[0m, in \u001b[0;36mcheck_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    110\u001b[0m     sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    113\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msizes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 227, 227, 30, 30\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define the video capture object\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the region of interest (ROI) coordinates\n",
    "x1, y1 = 150, 150\n",
    "x2, y2 = 550, 550\n",
    "\n",
    "# Load the grid points image\n",
    "grid_points = cv2.imread('grid_points.jpg', 1)\n",
    "\n",
    "# Define circle parameters\n",
    "radius = 10 \n",
    "color = (0, 255, 0)  # Green color (BGR format)\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video feed\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Crop the frame to the region of interest\n",
    "    frame_roi = frame[y1:y2, x1:x2]\n",
    "    \n",
    "    if not ret:\n",
    "        break \n",
    "        \n",
    "    # Extract and preprocess images from the frame\n",
    "    cropped_faces, original_images, left_eyes, right_eyes = extract_and_preprocess_images_grey(frame_roi)\n",
    "    \n",
    "    # Check if all four images are present\n",
    "    if cropped_faces and original_images and left_eyes and right_eyes:\n",
    "        # Predict gaze points using the model\n",
    "        points = model.predict([original_images, cropped_faces, left_eyes, right_eyes])\n",
    "        points = np.round(points).astype(int)\n",
    "        points = points[0]\n",
    "\n",
    "        # Create a copy of the grid_points image\n",
    "        grid_points_with_circles = grid_points.copy()\n",
    "\n",
    "        # Draw circles on the copy of grid_points\n",
    "        for point in points:\n",
    "            x, y = point\n",
    "            cv2.circle(grid_points_with_circles, (int(x), int(y)), radius, color, -1)\n",
    "\n",
    "        # Display the image with circles\n",
    "        cv2.imshow('Test Image with Circle', grid_points_with_circles)\n",
    "    \n",
    "    # Display the original frame\n",
    "    cv2.imshow('Video Feed', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
